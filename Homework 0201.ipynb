{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lr6Wlxltv6Z-"
      },
      "source": [
        "# Homework 2: Neural Network Training\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.random.seed(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### Definining the Neural Network Structure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KX4jyaK9LS07"
      },
      "source": [
        "**Exercise 1 [10/10]**:\n",
        "Define three values:\n",
        "- `n_x`: the size of the input data\n",
        "- `n_h`: the size of hidden layer, i.e., the number neurons in the hidden layer. The default value is $5$\n",
        "- `n_y`: the size of the output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pm3uIAqRImDa"
      },
      "outputs": [],
      "source": [
        "def neural_network_structure(X, Y, n_h):\n",
        "\n",
        "    n_x = X.shape[0]\n",
        "    n_y = Y.shape[0]\n",
        "    \n",
        "    return (n_x, n_h, n_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = np.random.randn(2, 3)\n",
        "Y = np.random.randn(1, 3)\n",
        "n_x, n_h, n_y = neural_network_structure(X, Y, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3keagCFJMrI3",
        "outputId": "8dda3d40-b848-4a54-8bc7-c03e0526f275"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The size of the input data: n_x = 2\n",
            "The size of the hidden layer: n_h = 10\n",
            "The size of the output: n_y = 1\n"
          ]
        }
      ],
      "source": [
        "print(\"The size of the input data: n_x = \" + str(n_x))\n",
        "print(\"The size of the hidden layer: n_h = \" + str(n_h))\n",
        "print(\"The size of the output: n_y = \" + str(n_y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### Random Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJ-hQEQHNZvm"
      },
      "source": [
        "**Excecise 2 [10/10]**: Implement the function `initialize_parameters()`.\n",
        "\n",
        "1. The function `initialize_parameters()` has input `n_x`, `n_h`, `n_y` as inputs.\n",
        "2. Use random normal distribution: `stdv * np.random.randn(a.b) + mu`, where `mu = 0.0` and `stdv = 1 / np.sqrt(n_x)` for `W_1` and `stdv = 1 / np.sqrt(n_h)` for `W_2`.\n",
        "3. Initialize biases as zeros with the correct shape: `np.zeros((a,b))`.\n",
        "4. Return `parameters` as a dictionary containing all weights and biases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "NRHewuF9MzYT"
      },
      "outputs": [],
      "source": [
        "def initialize_parameters(n_x, n_h, n_y):\n",
        "\n",
        "    W1 = np.random.randn(n_h, n_x) / np.sqrt(n_x)\n",
        "    b1 = np.zeros((n_h, 1))\n",
        "    W2 = np.random.randn(n_y, n_h) / np.sqrt(n_h)\n",
        "    b2 = np.zeros((n_y, 1))\n",
        "    \n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "    \n",
        "    return parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "parameters = initialize_parameters(n_x, n_h, n_y)\n",
        "W1, b1, W2, b2 = parameters['W1'], parameters['b1'], parameters['W2'], parameters['b2']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBD44YXmPESA",
        "outputId": "2cd4322c-f80f-42a6-dc85-ae43d37f4991"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "W1 = [[-0.17633148  1.03386644]\n",
            " [-1.45673947 -0.22798339]\n",
            " [-0.27156744  0.80169606]\n",
            " [-0.77774057 -0.12192515]\n",
            " [-0.62073964  0.02984963]\n",
            " [ 0.41211259 -0.77825528]\n",
            " [ 0.8094419   0.63752091]\n",
            " [ 0.35531715  0.63700135]\n",
            " [-0.48346861 -0.08689651]\n",
            " [-0.66168891 -0.18942548]]\n",
            "b1 = [[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "W2 = [[ 0.16771312 -0.21872233 -0.12546448 -0.21730309 -0.26727749 -0.21226666\n",
            "  -0.0040049  -0.35332456  0.07412875  0.52487553]]\n",
            "b2 = [[0.]]\n"
          ]
        }
      ],
      "source": [
        "print(\"W1 = \" + str(W1))\n",
        "print(\"b1 = \" + str(b1))\n",
        "print(\"W2 = \" + str(W2))\n",
        "print(\"b2 = \" + str(b2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### Sigmoid Function and Its Derivatives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAA8QsqdQe2t"
      },
      "source": [
        "**Exercise 3 [10/10]**:\n",
        "1. Implement sigmoid function `sigmoid()` as $\\sigma(x)=\\frac{1}{1+e^{-x}}$.\n",
        "2. Implement its derivative `sigmoid_derivative()` as $\\sigma^{\\prime}(x) = \\sigma(x) \\cdot (1-\\sigma(x))$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "RCiC1PCKPg2M"
      },
      "outputs": [],
      "source": [
        "def sigmoid_derivative(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = np.linspace(-5, 5, 10)\n",
        "s = sigmoid(x)\n",
        "s_d = sigmoid_derivative(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQL1_Pp2R1Cv",
        "outputId": "18395cbc-0b65-4df6-ed91-7546be8b9560"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.00669285 0.02005754 0.0585369  0.1588691  0.36457644 0.63542356\n",
            " 0.8411309  0.9414631  0.97994246 0.99330715]\n",
            "[0.00664806 0.01965523 0.05511033 0.13362971 0.23166046 0.23166046\n",
            " 0.13362971 0.05511033 0.01965523 0.00664806]\n"
          ]
        }
      ],
      "source": [
        "print(s)\n",
        "print(s_d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### Forward Propogation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6ty0zEmSQ9y"
      },
      "source": [
        " **Exercise 4 [10/10]**: Implementing forward propagation `forward_propagation()`\n",
        "\n",
        " 1. The function `forward_propagation()` takes `X` and `parameters` as inputs.\n",
        " 2. Retrieve the weights and bias from `parameters`.\n",
        " 3. Compute `Z1`, `A1`, `Z2`, and `A2` using the equations above.\n",
        " 4. Store intermediate variables in `cache` for use in backpropagation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "D8_VUXS3R4x6"
      },
      "outputs": [],
      "source": [
        "def forward_propagation(X, parameters):\n",
        "    \n",
        "    # Retrieve each parameter from the dictionary \"parameters\"\n",
        "    W1 = parameters[\"W1\"]\n",
        "    b1 = parameters[\"b1\"]\n",
        "    W2 = parameters[\"W2\"]\n",
        "    b2 = parameters[\"b2\"]\n",
        "\n",
        "    # Implement Forward Propagation to calculate A2\n",
        "    Z1 = W1 @ X + b1\n",
        "    A1 = sigmoid(Z1)\n",
        "    Z2 = W2 @ A1 + b2\n",
        "    A2 = sigmoid(Z2)\n",
        "    \n",
        "    # Store the intermedaite valeus in \"cache\" for backpropagation\n",
        "    cache = {\"Z1\": Z1,\n",
        "             \"A1\": A1,\n",
        "             \"Z2\": Z2,\n",
        "             \"A2\": A2}\n",
        "    \n",
        "    return A2, cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "A2, cache = forward_propagation(X, parameters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVayI3sdVaky",
        "outputId": "ae6417f3-c059-4166-ea8b-e2adc41b350b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-0.19151237249896635\n",
            "0.4688525159515502\n",
            "-0.3118444809339782\n",
            "0.42266909957970195\n"
          ]
        }
      ],
      "source": [
        "print(np.mean(cache['Z1']))\n",
        "print(np.mean(cache['A1']))\n",
        "print(np.mean(cache['Z2']))\n",
        "print(np.mean(cache['A2']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### Compute the Cost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3De3ZhSWHBj"
      },
      "source": [
        "**Exercise 5 [10/10]**: Implement `compute_cost()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "VCbyvFrbVxCQ"
      },
      "outputs": [],
      "source": [
        "def computer_cost(A2, Y):\n",
        "\n",
        "    m = Y.shape[1]\n",
        "    cost = (1 / (2 * m)) * np.sum((A2 - Y) ** 2)\n",
        "    \n",
        "    return cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuKAsSC2Wrz7",
        "outputId": "6d8ea601-34a3-441e-dbdd-5104c63950d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cost = 0.5239053069310721\n"
          ]
        }
      ],
      "source": [
        "print(f\"cost = {computer_cost(A2, Y)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### Back Propagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64bUYo3BW9Hm"
      },
      "source": [
        "**Exercise 6 [10/10]:** Implement `back_propogation()`\n",
        "1. The function `back_propogation()` takes data `X` and `Y`, weights and biases in `parameters`, and `cache` as inputs.\n",
        "2. Retrive weights (`W1` and `W_2`) and biase (`b1` and `b2`) from `parameters`.\n",
        "3. Retrive cached variables (`Z1`, `Z2`, `A1`, and `A2`) from `cache`.\n",
        "4. Compute the gradients `dW1`, `dW2`, `db1`, `db2` using the provided formulas and you may also need to compute `dZ2` and `dZ1` as needed.\n",
        "5. Return gradients in a variable `grads`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "-2FEJ7VkWzbR"
      },
      "outputs": [],
      "source": [
        "def back_propogation(X, Y, parameters, cache):\n",
        "\n",
        "    # Retrieve each parameter from the dictionary \"parameters\"\n",
        "    W1 = parameters[\"W1\"]\n",
        "    b1 = parameters[\"b1\"]\n",
        "    W2 = parameters[\"W2\"]\n",
        "    b2 = parameters[\"b2\"]\n",
        "\n",
        "    # Retrieve each value from the dictionary \"cache\"\n",
        "    Z1 = cache[\"Z1\"]\n",
        "    A1 = cache[\"A1\"]\n",
        "    Z2 = cache[\"Z2\"]\n",
        "    A2 = cache[\"A2\"]\n",
        "\n",
        "    # Compute gradients\n",
        "    m = Y.shape[1]\n",
        "    dZ2 = (A2 - Y) / m * sigmoid_derivative(Z2)\n",
        "    dW2 = dZ2 @ A1.T\n",
        "    db2 = np.sum(dZ2, axis = 1, keepdims = True)\n",
        "    dZ1 = (W2.T @ dZ2) * sigmoid_derivative(Z1)\n",
        "    dW1 = dZ1 @ X.T\n",
        "    db1 = np.sum(dZ1, axis = 1, keepdims = True)\n",
        "\n",
        "    # Stores the gradients\n",
        "    grads = {\"dW1\": dW1,\n",
        "             \"db1\": db1,\n",
        "             \"dW2\": dW2,\n",
        "             \"db2\": db2}\n",
        "\n",
        "    return grads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "grads = back_propogation(X, Y, parameters, cache)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WfKEbi9XYI9",
        "outputId": "93662664-9753-47a1-aae1-96f609cb7f9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dW1 = [[-0.00664941  0.00551946]\n",
            " [ 0.00666033 -0.00568816]\n",
            " [ 0.00529232 -0.00427319]\n",
            " [ 0.01028577 -0.00799248]\n",
            " [ 0.01305662 -0.0099341 ]\n",
            " [ 0.008174   -0.0066037 ]\n",
            " [ 0.00021912 -0.00017832]\n",
            " [ 0.02062341 -0.01630933]\n",
            " [-0.004029    0.00306173]\n",
            " [-0.02714012  0.02102039]]\n",
            "db1 = [[ 4.00223279e-04]\n",
            " [-2.71719312e-03]\n",
            " [-3.78546949e-04]\n",
            " [-1.13124221e-03]\n",
            " [-1.15711387e-03]\n",
            " [-9.91044386e-04]\n",
            " [-1.93340979e-06]\n",
            " [ 8.46922963e-04]\n",
            " [ 1.30854178e-04]\n",
            " [ 1.60536914e-03]]\n",
            "dW2 = [[ 0.04935606  0.05904166  0.04547199  0.0361181   0.03417343 -0.05364016\n",
            "  -0.01991294  0.00616847  0.02248469  0.02777064]]\n",
            "db2 = [[-0.0032622]]\n"
          ]
        }
      ],
      "source": [
        "print (\"dW1 = \" + str(grads[\"dW1\"]))\n",
        "print (\"db1 = \" + str(grads[\"db1\"]))\n",
        "print (\"dW2 = \" + str(grads[\"dW2\"]))\n",
        "print (\"db2 = \" + str(grads[\"db2\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### Update Weights and Biases Using Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa8E6b1OdvLO"
      },
      "source": [
        "**Exercise 7 [10/10]**: Implemente `update_parameters()`\n",
        "1. The function takes `parameters`, `grads`, and `learning_rate` as inputs.\n",
        "2. Retrive weights and biases from `parameters`.\n",
        "3. Retrive gradients from `grads`.\n",
        "4. Update the weights and biases using the gradient descent rule.\n",
        "5. Store the updated weights and biases back into `parameters` and return them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "B9GLhBWVXYLi"
      },
      "outputs": [],
      "source": [
        "def update_parameters(parameters, grads, learning_rate):\n",
        "\n",
        "    # Retrieve each parameter from the dictionary \"parameters\"\n",
        "    W1 = parameters[\"W1\"]\n",
        "    b1 = parameters[\"b1\"]\n",
        "    W2 = parameters[\"W2\"]\n",
        "    b2 = parameters[\"b2\"]\n",
        "\n",
        "    # Retrive gradients from the dictionary \"grads\"\n",
        "    dW1 = grads[\"dW1\"]\n",
        "    db1 = grads[\"db1\"]\n",
        "    dW2 = grads[\"dW2\"]\n",
        "    db2 = grads[\"db2\"]\n",
        "\n",
        "    # Update weights and biases using gradient descent update rule\n",
        "    W1 = W1 - learning_rate * dW1\n",
        "    b1 = b1 - learning_rate * db1\n",
        "    W2 = W2 - learning_rate * dW2\n",
        "    b2 = b2 - learning_rate * db2\n",
        "\n",
        "    # Store the updated weights and biases back into \"parameters\"\n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "\n",
        "    return parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "parameters = update_parameters(parameters, grads, 0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqjnXKjOXYNt",
        "outputId": "1041031b-689b-4f16-9302-3acc952173a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "W1 = [[-0.17626499  1.03381124]\n",
            " [-1.45680607 -0.22792651]\n",
            " [-0.27162036  0.80173879]\n",
            " [-0.77784343 -0.12184523]\n",
            " [-0.62087021  0.02994897]\n",
            " [ 0.41203085 -0.77818925]\n",
            " [ 0.80943971  0.6375227 ]\n",
            " [ 0.35511092  0.63716444]\n",
            " [-0.48342832 -0.08692713]\n",
            " [-0.66141751 -0.18963568]]\n",
            "b1 = [[-4.00223279e-06]\n",
            " [ 2.71719312e-05]\n",
            " [ 3.78546949e-06]\n",
            " [ 1.13124221e-05]\n",
            " [ 1.15711387e-05]\n",
            " [ 9.91044386e-06]\n",
            " [ 1.93340979e-08]\n",
            " [-8.46922963e-06]\n",
            " [-1.30854178e-06]\n",
            " [-1.60536914e-05]]\n",
            "W2 = [[ 0.16721956 -0.21931275 -0.1259192  -0.21766427 -0.26761923 -0.21173026\n",
            "  -0.00380577 -0.35338624  0.07390391  0.52459783]]\n",
            "b2 = [[3.262196e-05]]\n"
          ]
        }
      ],
      "source": [
        "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
        "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
        "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
        "print(\"b2 = \" + str(parameters[\"b2\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gb8WlNSfSD-"
      },
      "source": [
        "**Exercise 8 [10/10]**: Integrate the preivous parts into a function `train_loop()`.\n",
        "1. The function `train_loop()` takes input data `(X,Y)` and network size `n_h`, and `learning_rate` with `max_iteration` as inputs.\n",
        "2. Retrive `(n_x,n_h,n_y)` using `neural_network_structure()`.\n",
        "3. Initialize the parameters using `initialize_parameters()`.\n",
        "4. Create a `for` loop to train the network by calling `forward_propagation()` to compute the `cost` and `cach`, `back_propagation()` to compute the `grads`, then `update_parameters()` to update `parameters`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "n3rHawCvXYQB"
      },
      "outputs": [],
      "source": [
        "def train_loop(X, Y, n_h, learning_rate, max_iteration, print_cost = True):\n",
        "    \n",
        "    # Retrive (n_x, n_h, n_y)\n",
        "    n_x, n_h, n_y = neural_network_structure(X, Y, n_h)\n",
        "\n",
        "    # Initialize the parameters\n",
        "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
        "\n",
        "    for iter in range(max_iteration):\n",
        "        \n",
        "        # Forward propagation\n",
        "        A2, cache = forward_propagation(X, parameters)\n",
        "        \n",
        "        # Compute loss\n",
        "        cost = computer_cost(A2, Y)\n",
        "        \n",
        "        if print_cost:\n",
        "             print(f\"Epoch {iter}: Loss = {cost}\")\n",
        "\n",
        "        # Backward propagation\n",
        "        grads = back_propogation(X, Y, parameters, cache)\n",
        "\n",
        "        # Update parameters\n",
        "        parameters = update_parameters(parameters, grads, learning_rate)\n",
        "\n",
        "    return parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxhXILwThvKo",
        "outputId": "7330087c-6fe5-4b55-cb5e-9c948eeb10e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: Loss = 0.5413416581298804\n",
            "Epoch 1: Loss = 0.5411639794561593\n",
            "Epoch 2: Loss = 0.5409863314471564\n",
            "Epoch 3: Loss = 0.540808715111546\n",
            "Epoch 4: Loss = 0.5406311314554086\n",
            "Epoch 5: Loss = 0.5404535814821864\n",
            "Epoch 6: Loss = 0.5402760661926396\n",
            "Epoch 7: Loss = 0.5400985865848043\n",
            "Epoch 8: Loss = 0.5399211436539499\n",
            "Epoch 9: Loss = 0.5397437383925352\n"
          ]
        }
      ],
      "source": [
        "parameters = train_loop(X, Y, 10, 0.01, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### Training on Real Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Flatten the 28x28 images into vectors of 784 elements and normalize to [0, 1]\n",
        "X_train = X_train.reshape(X_train.shape[0], -1).T / 255.0\n",
        "X_test = X_test.reshape(X_test.shape[0], -1).T / 255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select only the samples of class '0' and '1' for binary classification\n",
        "train_filter = (y_train == 0) | (y_train == 1)\n",
        "test_filter = (y_test == 0) | (y_test == 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_binary = X_train[:, train_filter]\n",
        "y_train_binary = y_train[train_filter].reshape(1, -1)  # Reshape to (1, num_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_test_binary = X_test[:, test_filter]\n",
        "y_test_binary = y_test[test_filter].reshape(1, -1)  # Reshape to (1, num_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data shape: (784, 12665)\n",
            "Training labels shape: (1, 12665)\n",
            "Testing data shape: (784, 2115)\n",
            "Testing labels shape: (1, 2115)\n",
            "Training labels: [0 1]\n",
            "Testing labels: [0 1]\n"
          ]
        }
      ],
      "source": [
        "print(f\"Training data shape: {X_train_binary.shape}\")\n",
        "print(f\"Training labels shape: {y_train_binary.shape}\")\n",
        "print(f\"Testing data shape: {X_test_binary.shape}\")\n",
        "print(f\"Testing labels shape: {y_test_binary.shape}\")\n",
        "print(\"Training labels:\", np.unique(y_train_binary))\n",
        "print(\"Testing labels:\", np.unique(y_test_binary))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select a few random indices\n",
        "indices = np.random.choice(X_train_binary.shape[1], size = 5, replace = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "Y5RgdRBx5qBz",
        "outputId": "9bac023f-4d20-4b69-cbb4-5061011a12b6"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x200 with 5 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plot the images\n",
        "fig, axes = plt.subplots(1, 5, figsize = (10, 2))\n",
        "\n",
        "for i, idx in enumerate(indices):\n",
        "    image = X_train_binary[:, idx].reshape(28, 28)\n",
        "    label = y_train_binary[0, idx]\n",
        "    axes[i].imshow(image, cmap = 'gray')\n",
        "    axes[i].set_title(f\"Label: {label}\")\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8qLerYChvP2",
        "outputId": "c217504d-1fbf-4665-947a-5877e525b09c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: Loss = 0.15423868236507246\n",
            "Epoch 1: Loss = 0.1538372075502308\n",
            "Epoch 2: Loss = 0.1534339482553399\n",
            "Epoch 3: Loss = 0.15302893320861544\n",
            "Epoch 4: Loss = 0.15262219214320855\n",
            "Epoch 5: Loss = 0.15221375579440194\n",
            "Epoch 6: Loss = 0.15180365589554054\n",
            "Epoch 7: Loss = 0.15139192517266983\n",
            "Epoch 8: Loss = 0.15097859733785732\n",
            "Epoch 9: Loss = 0.1505637070811758\n"
          ]
        }
      ],
      "source": [
        "parameters = train_loop(X_train_binary, y_train_binary, 10, 0.01, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### Predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfb-_P45m4K2"
      },
      "source": [
        "**Exercise 9 [10/10]:**\n",
        "1. The function `predict()` takes `X` and `parameters` as inputs.\n",
        "2. Call `forward_propagation()` to obtain the output `A2`.\n",
        "3. Assign labels using threshold `0.5`; label class `1` if `A2 > 0.5`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "QcAuTkBghvST"
      },
      "outputs": [],
      "source": [
        "def predict(X, parameters):\n",
        "\n",
        "    A2, cache = forward_propagation(X, parameters)\n",
        "    predictions = (A2 > 0.5).astype(int)\n",
        "    \n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_predictions = predict(X_train_binary, parameters)\n",
        "test_predictions = predict(X_test_binary, parameters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iijKBDURohhx",
        "outputId": "230c6b79-11aa-4000-f9cb-8218fcdfc2d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Accuracy: 0.5323332017370707\n",
            "Testing Accuracy: 0.5366430260047281\n"
          ]
        }
      ],
      "source": [
        "print(f\"Training Accuracy: {np.mean(train_predictions == y_train_binary)}\")\n",
        "print(f\"Testing Accuracy: {np.mean(test_predictions == y_test_binary)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### Tuning Network Hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhpyVaKLow0V"
      },
      "source": [
        "**Exercise 10 [10/10]**: Experiment with different values for network size `n_h` to observe how the network size influences performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "network_sizes = [5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUlGtGoVp84g",
        "outputId": "6227bac2-dc80-4d71-f90c-6d8455849369"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Network Size: 5, Training Accuracy: 0.5323332017370707, Testing Accuracy: 0.5366430260047281\n",
            "Network Size: 10, Training Accuracy: 0.46766679826292934, Testing Accuracy: 0.46335697399527187\n",
            "Network Size: 20, Training Accuracy: 0.46766679826292934, Testing Accuracy: 0.46335697399527187\n",
            "Network Size: 30, Training Accuracy: 0.4671930517173312, Testing Accuracy: 0.4628841607565012\n",
            "Network Size: 40, Training Accuracy: 0.5323332017370707, Testing Accuracy: 0.5366430260047281\n",
            "Network Size: 50, Training Accuracy: 0.5323332017370707, Testing Accuracy: 0.5366430260047281\n",
            "Network Size: 60, Training Accuracy: 0.5323332017370707, Testing Accuracy: 0.5366430260047281\n",
            "Network Size: 70, Training Accuracy: 0.5705487564153178, Testing Accuracy: 0.5839243498817966\n",
            "Network Size: 80, Training Accuracy: 0.484247927358863, Testing Accuracy: 0.4983451536643026\n",
            "Network Size: 90, Training Accuracy: 0.5323332017370707, Testing Accuracy: 0.5366430260047281\n",
            "Network Size: 100, Training Accuracy: 0.5267272009474931, Testing Accuracy: 0.5295508274231678\n"
          ]
        }
      ],
      "source": [
        "for n_h in network_sizes:\n",
        "    parameters = train_loop(X_train_binary, y_train_binary, n_h, 0.01, 10, False)\n",
        "    train_predictions = predict(X_train_binary, parameters)\n",
        "    test_predictions = predict(X_test_binary, parameters)\n",
        "    train_accuracy = np.mean(train_predictions == y_train_binary)\n",
        "    test_accuracy = np.mean(test_predictions == y_test_binary)\n",
        "    print(f\"Network Size: {n_h}, Training Accuracy: {train_accuracy}, Testing Accuracy: {test_accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
